{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a407b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import gym\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple, BaseEnv\n",
    "from typing import Dict, NamedTuple, List\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "train_mode = True # Whether to run the environment in training or inference mode\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b672fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "  \"\"\"\n",
    "  An experience contains the data of one Agent transition.\n",
    "  - Observation\n",
    "  - Action\n",
    "  - Reward\n",
    "  - Done flag\n",
    "  - Next Observation\n",
    "  \"\"\"\n",
    "\n",
    "  obs: np.ndarray\n",
    "  action: np.ndarray\n",
    "  reward: float\n",
    "  done: bool\n",
    "  next_obs: np.ndarray\n",
    "\n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561d896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(model, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0cf454b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    @staticmethod\n",
    "    def generate_trajectories(\n",
    "        env: BaseEnv, q_net: model, buffer_size: int, epsilon: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Given a Unity Environment and a Q-Network, this method will generate a\n",
    "        buffer of Experiences obtained by running the Environment with the Policy\n",
    "        derived from the Q-Network.\n",
    "        :param BaseEnv: The UnityEnvironment used.\n",
    "        :param q_net: The Q-Network used to collect the data.\n",
    "        :param buffer_size: The minimum size of the buffer this method will return.\n",
    "        :param epsilon: Will add a random normal variable with standard deviation.\n",
    "        epsilon to the value heads of the Q-Network to encourage exploration.\n",
    "        :returns: a Tuple containing the created buffer and the average cumulative\n",
    "        the Agents obtained.\n",
    "        \"\"\"\n",
    "        # Create an empty Buffer\n",
    "        buffer: Buffer = []\n",
    "\n",
    "        # Reset the environment\n",
    "        env.reset()\n",
    "        # Read and store the Behavior Name of the Environment\n",
    "        behavior_name = env.action_space.sample()\n",
    "\n",
    "        # Create a Mapping from AgentId to Trajectories. This will help us create\n",
    "        # trajectories for each Agents\n",
    "        dict_trajectories_from_agent = self.Trajectory\n",
    "        # Create a Mapping from AgentId to the last observation of the Agent\n",
    "        dict_last_obs_from_agent = np.ndarray\n",
    "        # Create a Mapping from AgentId to the last observation of the Agent\n",
    "        dict_last_action_from_agent = np.ndarray\n",
    "        # Create a Mapping from AgentId to cumulative reward (Only for reporting)\n",
    "        dict_cumulative_reward_from_agent:\n",
    "        # Create a list to store the cumulative rewards obtained so far\n",
    "        cumulative_rewards: List[float] = []\n",
    "        \n",
    "        while len(buffer) < buffer_size:  # While not enough data in the buffer\n",
    "            \n",
    "            # Get the Decision Steps and Terminal Steps of the Agents\n",
    "            observation, reward, done, _ = env.step(behavior_name)\n",
    "            onState = done\n",
    "            \n",
    "            # For all Agents with a Terminal Step:\n",
    "            if not onState: # on Terminal State\n",
    "\n",
    "                # Create its last experience (is last because the Agent terminated)\n",
    "                last_experience = Experience(\n",
    "                    obs=observation,\n",
    "                    reward=reward,\n",
    "                    done=done,\n",
    "                    action=behavior_name,\n",
    "                    next_obs=terminal_steps[agent_id_terminated].obs[0],\n",
    "                )\n",
    "\n",
    "                \"\"\"\n",
    "                # Clear its last observation and action (Since the trajectory is over)\n",
    "                dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "                dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "                \"\"\"\n",
    "\n",
    "                # Report the cumulative reward\n",
    "                cumulative_reward = (\n",
    "                    dict_cumulative_reward_from_agent.pop(agent_id_terminated)\n",
    "                    + terminal_steps[agent_id_terminated].reward\n",
    "                )\n",
    "                cumulative_rewards.append(cumulative_reward)\n",
    "                # Add the Trajectory and the last experience to the buffer\n",
    "                buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "                buffer.append(last_experience)\n",
    "\n",
    "            # For all Agents with a Decision Step:\n",
    "            else:\n",
    "                # If the Agent does not have a Trajectory, create an empty one\n",
    "                if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "                    dict_trajectories_from_agent = []\n",
    "                    dict_cumulative_reward_from_agent = 0\n",
    "\n",
    "                # Create an Experience from the last observation and the Decision Step\n",
    "                exp = Experience(\n",
    "                    obs=observation,\n",
    "                    reward=reward,\n",
    "                    done=False,\n",
    "                    action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
    "                    next_obs=,\n",
    "                )\n",
    "                # Update the Trajectory of the Agent and its cumulative reward\n",
    "                dict_trajectories_from_agent.append(exp)\n",
    "                dict_cumulative_reward_from_agent += (\n",
    "                    reward\n",
    "                )\n",
    "                # Store the observation as the new \"last observation\"\n",
    "\n",
    "                dict_last_obs_from_agent = observation\n",
    "        \n",
    "            # Compute the values for each action given the observation\n",
    "            actions_values = (\n",
    "                q_net(torch.from_numpy(decision_steps.obs[0])).detach().numpy()\n",
    "            )\n",
    "            \n",
    "            if random.uniform(0, 1) <= epsilon:\n",
    "                actions = np.random.randint(3, size=(decision_steps.agent_id.shape[0], 1))\n",
    "            else:\n",
    "                actions = np.argmax(actions_values, axis=1)\n",
    "                actions.resize((len(decision_steps), 1))\n",
    "            \n",
    "            # Store the action that was picked, it will be put in the trajectory later\n",
    "            for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "                dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
    "\n",
    "            # Set the actions in the environment\n",
    "            # Unity Environments expect ActionTuple instances.\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_discrete(actions)\n",
    "            \n",
    "            # Perform a step in the simulation\n",
    "            env.set_actions(behavior_name, action_tuple)\n",
    "            env.step()\n",
    "        return buffer, np.mean(cumulative_rewards)\n",
    "\n",
    "    @staticmethod\n",
    "    def update_q_net(\n",
    "        q_net: model,\n",
    "        optimizer: torch.optim,\n",
    "        buffer: Buffer,\n",
    "        action_size: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs an update of the Q-Network using the provided optimizer and buffer\n",
    "        \"\"\"\n",
    "        BATCH_SIZE = 1000\n",
    "        NUM_EPOCH = 3\n",
    "        GAMMA = 0.9\n",
    "        batch_size = min(len(buffer), BATCH_SIZE)\n",
    "        random.shuffle(buffer)\n",
    "        # Split the buffer into batches\n",
    "        batches = [\n",
    "            buffer[batch_size * start : batch_size * (start + 1)]\n",
    "            for start in range(int(len(buffer) / batch_size))\n",
    "        ]\n",
    "        \n",
    "        for _ in range(NUM_EPOCH):\n",
    "            for batch in batches:\n",
    "                # Create the Tensors that will be fed in the network\n",
    "                obs = torch.from_numpy(np.stack([ex.obs for ex in batch]))\n",
    "                reward = torch.from_numpy(\n",
    "                    np.array([ex.reward for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "                )\n",
    "                done = torch.from_numpy(\n",
    "                    np.array([ex.done for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "                )\n",
    "                action = torch.from_numpy(np.stack([ex.action for ex in batch]))\n",
    "                next_obs = torch.from_numpy(np.stack([ex.next_obs for ex in batch]))\n",
    "                \n",
    "                # Use the Bellman equation to update the Q-Network\n",
    "                target = (\n",
    "                    reward\n",
    "                    + (1.0 - done)\n",
    "                    * GAMMA\n",
    "                    * torch.max(q_net(next_obs).detach(), dim=1, keepdim=True).values\n",
    "                )\n",
    "\n",
    "                mask = torch.zeros((len(batch), action_size))\n",
    "                mask.scatter_(1, action, 1)\n",
    "                prediction = torch.sum(q_net(obs) * mask, dim=1, keepdim=True)\n",
    "                criterion = torch.nn.SmoothL1Loss()\n",
    "                loss = criterion(prediction, target)\n",
    "\n",
    "                # Perform the backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d177731a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityCommunicatorStoppedException",
     "evalue": "Communicator has exited.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityCommunicatorStoppedException\u001b[0m         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3200/949068231.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mexperiences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_exp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_q_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_trajectories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mcumulative_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training step \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\treward \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3200/2736007924.py\u001b[0m in \u001b[0;36mgenerate_trajectories\u001b[1;34m(env, q_net, buffer_size, epsilon)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;31m# Perform a step in the simulation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbehavior_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcumulative_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages\\mlagents_envs\\timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages\\mlagents_envs\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll_process\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityCommunicatorStoppedException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Communicator has exited.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_behavior_specs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mrl_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrl_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnityCommunicatorStoppedException\u001b[0m: Communicator has exited."
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "# -----------------\n",
    "\n",
    "# Create the GridWorld Environment from the registry\n",
    "env.reset()\n",
    "\n",
    "num_actions = 3\n",
    "num_obs = 9\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "\n",
    "try:\n",
    "    # Create a new Q-Network.\n",
    "    qnet = model(num_obs, num_actions)\n",
    "\n",
    "    experiences: Buffer = []\n",
    "    optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
    "\n",
    "    cumulative_rewards: List[float] = []\n",
    "\n",
    "    # The number of training steps that will be performed\n",
    "    NUM_TRAINING_STEPS = int(os.getenv('QLEARNING_NUM_TRAINING_STEPS', 30))\n",
    "    # The number of experiences to collect per training step\n",
    "    NUM_NEW_EXP = int(os.getenv('QLEARNING_NUM_NEW_EXP', 10000))\n",
    "    # The maximum size of the Buffer\n",
    "    BUFFER_SIZE = int(os.getenv('QLEARNING_BUFFER_SIZE', 10000))\n",
    "\n",
    "    for n in range(NUM_TRAINING_STEPS):\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * n / EPS_DECAY)\n",
    "        new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=eps_threshold)\n",
    "        random.shuffle(experiences)\n",
    "        if len(experiences) > BUFFER_SIZE:\n",
    "            experiences = experiences[:BUFFER_SIZE]\n",
    "        experiences.extend(new_exp)\n",
    "        Trainer.update_q_net(qnet, optim, experiences, num_actions)\n",
    "        _, rewards = Trainer.generate_trajectories(env, qnet, 1000, epsilon=0)\n",
    "        cumulative_rewards.append(rewards)\n",
    "        print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted, continue to next cell to save to save the model.\")\n",
    "finally:\n",
    "    env.close()\n",
    "\n",
    "# Show the training graph\n",
    "try:\n",
    "    plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)\n",
    "except ValueError:\n",
    "    print(\"\\nPlot failed on interrupted training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44cc5f34",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Discrete' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4776/2719706602.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# take a random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Discrete' object is not callable"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(10):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    print(env.action_space(0))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb096a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
