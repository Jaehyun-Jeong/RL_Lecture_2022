![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/Introduction_to_RL%2F2.1%2Fthumbnail.png?alt=media&token=7b97bf80-2368-4dbf-95d9-5e677baa4937)

---
# 무작정 따라하는 강화학습 프로젝트
---

## 1. Introduction

강화학습은 **환경**(**Environment**)과 **에이전트**(**Agent**)의 상호작용이다.

이번 챕터에서는 이러한 *환경*으로 파이썬의 gym 모듈들 사용하고 *Deep q-learning*(*DQN*)을 학습 알고리즘으로 사용하여, 간단한 모델을 생성하여 학습하고 결과를 보겠다.
하지만, 알고리즘을 이해하기 위해서는 강화학습과 *DQN* 알고리즘을 이해할 필요가 있다.
따라서 강화학습의 기본 개념을 시작으로 *DQN*알고리즘을 알아보고, 배운 개념을 Python 코드로 짜보려고 한다.
마지막으로는 학습의 결과를 확인하고, 배운 개념을 정리하면서 끝맺도록 하겠다.

---

## 2. 강화학습의 기본 개념

### 1) 강화학습의 목표와 **Bellman Equation**

#### **Reward Hypothesis**에 의하면 에이전트의 목표는 보상 축적값의 최대화이다.

보상의 최대화가 아닌 보상 축적값의 최대화가 목표인 이유가 무엇인지 생각해 볼 필요가 있다.

> 예를 들어, 기업의 이윤을 최대화하는 에이전트를 생각해보자. 이때 나는 에이전트를 기업의 경영자 A라 가정하겠다.A의 목표는 1년동안의 이윤을 최대화 하기이다. 하지만 A는 여름 계절상품을 기획하였고, 기업의 여름 이윤은 급 상승 하였지만 다른 계절에는 좋은 성과를 만들지 못하였다. 이로 인해, 기업의 1년간 이윤은 평균적으로 낮아졌고 A는 목표를 달성하지 못했다. 여기서 A의 실패 요인은 *근시안적*인 목표 달성에 중점을 두었다는 것이다. 따라서 A가 성공하기 위해서는 최소 1년동안 이윤을 보장하는 기획이 필요하다.

위의 예를 보면 알 수 있듯이, 바로 얻을 수 있는 *보상*(*여름 동안의 기업이윤*)만을 고려하게 되면 *보상의 축적값*(*1년 동안 축적된 기업이윤*)이라는 목표를 달성할 수 없다. 따라서, 에이전트는 *근시안적*인 단일 보상이 아닌, 보상의 축적값을 최대화 할 수 있는 방법을 찾아야한다.

#### 이러한 축적된 보상을 Return이라 하고, 다음과 같이 정의한다.

$ G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_T $

* $t$ : *time-step* $t$는 에이전트와 환경의 상호작용을 순서대로 구분할 때 사용하는 인덱스이다.
* $T$ : *Terminal time-step* $T$는 에이전트와 환경의 상호작용이 자연스럽게 종료되는 *time-step* 인덱스이다. 물론, T가 무한인 경우도 존재한다.
* $R_t$ : *time-step* $t$에서의 **보상**(**Reward**)
* $G_t$ : *time-step* $t$에서부터 $T$까지의 **축적된 보상**(**Return**)

에이전트의 목표인 *Return*의 최대화를 바탕으로, 강화학습의 목표를 다음과 같이 설명할 수 있다.

#### 에이전트가 *Return*을 최대화 하는 행동을 선택할 때, 강화학습의 목표가 달성된다.

즉, 강화학습은 에이전트가 최적의 행동을 하도록 학습시킨다.

위같은 조건을 만족하기 위해서, 특정 상태의 가치와 그때 선택되는 행동의 가치를 판단할 필요가 있는데, 이를 결정하는 함수를 **Value Function**이라 하고 다음과 같이 정의한다.

**State-value Function** : $ v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E} \left[ \displaystyle\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s \right]\,\, , \forall s \isin \mathcal{S} $

**Action-value Function** : $ q_\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a] = \mathbb{E}_\pi \left[ \displaystyle\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a \right]\,\, , \forall s \isin \mathcal{S}, a \isin \mathcal{A} $

* $\pi$ : 에이전트의 *Policy*
* $s$ : 환경에서 얻어진 *상태(State)* $s$
* $\gamma$ : *step-size* $\gamma$는 미래 보상의 반영도를 결정한다. e.g. $\gamma$가 0이면 $\displaystyle\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} = R_{t+1}$이므로 즉각적인 보상만을 고려하게 되고, 1이면 $\displaystyle\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} = \displaystyle\sum_{k=0}^{\infty} R_{t+k+1}$이므로 먼 미래의 보상과 즉각적인 보상을 같은 정도로 고려하게 된다.
* $v_\pi(s)$ : *Policy* $\pi$를 따라갈 때, *State* $s$에서의 가치
* $q_\pi(s, a)$ : *Policy* $\pi$를 따라갈 때, *State* $s$에서 *Action* $a$를 선택하면 얻을 수 있는 가치

위의 개념을 바탕으로 다음과 같이 정의한 방정식을 **Bellman Equation**이라 한다.

$ v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] $ 

$ = \mathbb{E}_\pi [R_{t+1} + \gamma G_{t+1} | S_t = s] $

$ = \displaystyle\sum_a \pi(a|s) \displaystyle\sum_{s'} \displaystyle\sum_r p(s', r| s, a) \left[ r + \gamma \mathbb{E}_\pi [G_{t+1} | S_{t+1} = s'] \right] $

$ = \displaystyle\sum_a \pi(a|s) \displaystyle\sum_{s', r} p(s', r|s, a) \left[ r + \gamma v_\pi(s') \right] $

$ \therefore v_\pi(s) = \displaystyle\sum_a \pi(a|s) \displaystyle\sum_{s', r} p(s', r|s, a) \left[ r + \gamma v_\pi(s') \right] $

#### Bellman Equation 은 전 상태($s$)와 후 상태($s'$)만을 사용하여 가치함수를 설명한다.

### 2) Q_learning 알고리즘이란?

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F01.png?alt=media&token=32c3cb62-c70a-468a-92a1-b77889ffc6ee)
*Q-learning의 의사코드*
*출처: Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.*

#### off-policy TD(Temporal Difference) control이란?

우선 **off-policy**와**on-policy**는 다음과 같은 방법이다.
* *on-policy*는 학습에 필요한 데이터를 얻은 *Policy*와 실제로 학습되는 *Policy*로 같은 *Policy*를 사용하는 방법이다.
* *off-policy*는 *on-policy*와 달리 데이터 수집과 실제 학습에 다른 *Policy*를 사용한다.

> 예를 들어, 자동차 에이전트를 학습시킨다고 생각하자. *on-policy*의 경우, 항상 가치가 높은 행동을 선택하게 되면 새로운 경로를 탐색할 수 없어진다. 하지만 *off-policy*를 사용하여 학습에 필요한 데이터를 얻기 위해 랜덤하게 행동을 선택하는 *Policy*를 사용하면 여러 경로를 탐색할 수 있게되고, 가치가 높은 행동을 선택하는 *Policy*를 학습시키면 모든 경로를 탐색할 수 있게 된다.
> 하지만 *on-policy*도 여러 경로를 탐색 못하는 것은 아니다. 예를 들어, *Q-learning* 의사코드에도 제시된 **$\epsilon$-greedy Policy**의 경우 $\epsilon$의 확률로 랜덤한 행동을 선택하고, $1-\epsilon$의 확률로 가치가 가장 높은 행동을 선택한다.

강화학습을 하기 위해 환경으로부터 데이터를 수집하고 가치를 수정하는 방법은 여러가지 있지만 간단히 3개로 나뉘어 진다.

1. 하나의 *Episode*를 진행한 후, 얻어진 데이터로 가치를 수정한다.
2. $T$번의 *time-step*후, 얻어진 데이터로 가치를 수정한다.
3. 한번의 *time-step*후, 얻어진 데이터로 가치를 수정한다.

여기서 **Temporal Diffrence (TD)** 는 이름에서도 예상할 수 있듯이 한번의 *time-step*후 가치를 수정한다. 즉, 에이전트가 행동을 선택하고 보상을 얻을 때마다 가치를 수정한다.

마지막으로 **control**은 얻어진 정보를 바탕으로 *Policy*를 수정함을 의미한다.

#### 한 줄씩 알고리즘 해석하기

위의 개념을 바탕으로 한 줄씩 알고리즘을 해석하자.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F02.png?alt=media&token=02517712-4bdf-4a13-b92e-a9a24a64d583)

1. *step-size* $\alpha$와 *$\epsilon$-greedy Policy*를 위한 $\epsilon$을 선택한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F03.png?alt=media&token=345086c9-8405-4d05-9fa9-f60f34872b86)

2. 임의의 방법으로 모든 $s$와 행동 $a$에 대하여 $Q(s, a)$를 초기화한다.
$s$가 *Episode*의 마지막(*Terminal*)이면 $Q(s, a) = 0$

* $\mathcal{S}$ : *Terminal*을 제외한 모든 상태들의 집합
* $\mathcal{S}^+$ : *Terminal*을 포함한 모든 상태들의 집합
* $\mathcal{A}(s)$ : 상태 $s$에서 선택 가능한 모든 행동의 집합

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F04.png?alt=media&token=30c38ae7-33c6-4b9a-bbad-0d1b65bf37db)

3. *Episode*마다 안의 내용을 반복한다.

> * 여기서 '안'이란 파이썬 코드와 비슷하게 들여쓰기 한 부분을 의미한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F05.png?alt=media&token=29d9be0b-30b5-4e92-a436-03ff9bf82f47)

4. 임의의 방법으로 처음 상태 $S$를 선택한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F06.png?alt=media&token=66488a68-b3fc-44b5-bb6e-da182653a85b)

5. 각 *step*마다 안에 있는 내용을 반복한다.

* step: 에이전트가 상태를 바탕으로 행동하고 보상을 받고 다음 상태를 확인하는 일련의 과정

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F07.png?alt=media&token=54a47e5e-6ff4-4201-bc4b-bebcc8513ff0)

6. $Q$를 사용한 *Policy*에 대하여 행동 *A*를 선택한다.

> * 여기서 *off-policy*가 보장되는 이유는 학습되는 *Policy*는 $Q$에 대한 *$\epsilon$-greedy Policy*가 아닌 *greedy Policy*를 선택하기 때문이다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F08.png?alt=media&token=aa87e461-090b-4fac-b6b9-d3cfcc0a138c)

7. 에이전트는 선택된 행동을 실행하고 결과를 관찰한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F09.png?alt=media&token=1f13afd6-d6ea-4819-9840-c8d68896e0c6)

8. 이는 $Q(S,A)$를 업데이트 하는 과정이다.

보면 *Gradient Descent*와 비슷한 모양을 하고 있는데,
**$R + \gamma \underset{a}{\mathrm{max}}\, Q(S',a)$는 타켓이고,**
**$[R + \gamma \underset{a}{\mathrm{max}}\, Q(S',a) - Q(S,A)]$는에러로 해석할 수 있다.**

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F10.png?alt=media&token=b2b49632-9915-40c4-8c64-b91a7c16cd45)

9. 다음 상태를 지금 상태에 저장한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F11.png?alt=media&token=720fd1ad-41a2-4e91-b8fb-daae7397c234)

10. 위 과정을 *Terminal* 상태에 도달할 때(*Episode*가 끝날 때) 까지 진행한다.

### 3) Deep q-learning이란?

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F27.png?alt=media&token=2d2b0465-984f-40d3-8582-6b8bf3391731)
*Deep Q-learning의 의사코드*
*출처: Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (Dec 2013). Playing Atari with deep reinforcement learning. Technical Report arXiv:1312.5602 [cs.LG], Deepmind Technologies.*

#### 위에서 Q-learning 알고리즘을 살펴보았다. 이를 뉴럴 네트워크, 그리고 Gradient Descent와 어떻게 결합했는지 살펴보도록 하겠다.

위에서도 알 수 있듯이 *Action Value*는 다음과 같이 정의된다.

$ v(s, a) =  \mathbb{E}_\pi \left[ \displaystyle\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a \right]\,\, , \forall s \isin \mathcal{S}, a \isin \mathcal{A} $

**하지만 Deep q-learning에서는 이러한 가치함수와 뉴럴 네트워크를 결합한 함수를 다음과 같이 표현한다.**

#### $ Q(\phi, a; \theta) $

이 함수는 가중치가 $\theta$인 뉴럴 네트워크에 대한 행동의 가치함수이다. 하지만 상태 $s$가 보이지 않는다. 대신에 $\phi$가 보이는데, 이 $\phi$는 다음과 같다.

#### $\phi$는 상태를 전처리하는 함수이다.

> * 예를 들어, 논문에서는 아타리 게임을 학습시키는데 $\phi(s)$는 게임 화면의 이미지인 $s$를 전처리 하는 함수이고 다음과 같은 과정을 거친다.
> 1. 이미지를 흑백(gray-scale)로 변경한다.
> 2. 이미지 사이즈를 110x84 픽셀로 줄인다. (**down-sampling**)
> 3. 실제 플레이과 관련 있는 부분을 84x84 픽셀로 잘라낸다. (**cropping**)
> 4. 이러한 방법으로 얻어진 이미지를 4개 가중시켜 $\phi(s)$에 저장한다.

이제 $\phi$를 이해했다. 다음으로 뉴럴 네트워크는 어떻게 결합되었는지 생각할 수 있는데, 이는 다음 문장을 보면 알 수 있다.

#### Deep q-learning* 에서는 *Policy*(상태와 행동에 대한 확률 분포)가 뉴럴 네트워크이다. 즉, Policy에 환경의 상태값을 인풋하면 에이전트 행동의 확률을 리턴한다.

지금까지 *Deep q-learning* 알고리즘 이해를 위한 바탕을 모두 공부하였다.

**이제부터 알고리즘을 한줄 씩 분석해 보겠다.**

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F12.png?alt=media&token=badf35c2-1a0a-41a4-8be0-1cd80ed9f163)

1. 사이즈가 $N$인 *Replay Memory* $\mathcal{D}$를 초기화한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F13.png?alt=media&token=238e0584-9823-440b-bdf0-a76ba8878c73)

2. 랜덤한 가중치인 $\theta$에 대하여 행동 가치함수 $Q$를 초기화한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F14.png?alt=media&token=24a69cb3-a568-40ae-9a33-38ff5e704ad4)

3. $M$개의 *Episode*에 대하여 안의 내용을 반복한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F15.png?alt=media&token=2f5ce349-05a3-4d7c-93aa-decc79765e4f)

4. 데이터열 $s_1 = {x_1}$과 이를 전처리 한 $\phi_1 = \phi(s_1)$를 초기화 한다.

* $x_1$ : 환경에서 얻어진 상태이다. 

> * 전 예문에서도 봤듯이 아타리 게임의 경우 $x_1$은 처음 얻어진 아타리 게임의 화면 이미지이다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F16.png?alt=media&token=727c96c4-b3e8-4cc4-b1a5-829ba52a2c6d)

5. *step*마다 안의 내용을 반복한다.

* $T$ :  *Terminal State*

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F17.png?alt=media&token=065b419c-b4e2-45bc-8990-c646f0c611ad)

6. $\epsilon$의 확률로 랜덤한 행동 $a_t \isin \mathcal{A}(s_t)$을 선택한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F18.png?alt=media&token=2092e7a9-870c-452e-975e-eea658506c21)

7. $1-\epsilon$의 확률로 *greedy*한 행동을 선택한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F19.png?alt=media&token=82322268-69aa-43f1-8626-83cff70f9fa5)

8. 행동 $a_t$를 취한 후, 환경(*emulator*)에서 보상 $r_t$와 상태(*image*) $x_{t+1}$를 가져온다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F20.png?alt=media&token=3283ba24-4bca-4d1f-a833-79691c2d0024)

9. $s_{t+1}$에 데이터열 $s_t, a_t, x_{t+1}$을 저장하고, 다음과 같은 방식으로 전처리 한다.
$\phi_{t+1} = \phi(s_{t+1})$

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F21.png?alt=media&token=0e2fadc7-790e-4312-ad59-c7d4b51a3265)

10. 0*Replay Memory*에 $(\phi_t, a_t, r_t, \phi_{t+1})$를 저장한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F22.png?alt=media&token=8ff6bf03-991d-4de7-8929-9e05ef2de280)

11. *Replay Memory* $\mathcal{D}$에서 랜덤한 *minibatch*를 추출한다.

> * 이러한 방법을 사용하는 이유는 데이터의 독립성을 보장하기 때문이다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F23.png?alt=media&token=ae1356d9-434b-4582-adda-4f13a7ec6116)

12. $\phi_{j+1}$가 *Terminal*이면 $\gamma \underset{a'}{\mathrm{max}} Q(\phi_{j+1}, a'; \theta) = 0$ 이므로 $y_j$는 위와 같이 정의된다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F24.png?alt=media&token=1843ad99-930c-4fbc-b0ae-5488a07cf6ed)

**Loss Function**은 다음과 같이 정의한다.
$ L_i(\theta_i) = \mathbb{E}_{s, a \sim \rho(\cdot)} \left[ (y_i - Q(s, a; \theta_i))^2 \right] $

이러한 관점에서 위의 **equation 3**은 다음과 같다.
$ \nabla_{\theta_i} L_i(\theta_i) = \mathbb{E}_{s, a \sim \rho(\cdot); s' \sim \varepsilon} \left[ \left( r + \gamma \underset{a'}{\mathrm{max}}\, Q(s', a' ; \theta_{i-1}) - Q(s, a; \theta_i) \right) \nabla_{\theta_i} Q(s, a; \theta_i) \right] $

13. 이제 다음과 같이 *Gradient Descent*를 실행한다.
$ \forall i,\,\, \theta_i = \theta_i - \alpha \nabla_{\theta_i} L_i(\theta_i) $

* $\alpha$ : *learning rate* 또는 *step-size* (같은 표현이다.)

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F25.png?alt=media&token=ad0c5e7e-b571-47c8-928e-a74b0fe87b0b)

14. $t = T$이면 안의 반복을 종룍한다.

![](https://firebasestorage.googleapis.com/v0/b/aing-biology.appspot.com/o/2022_RL_Lecture%2F26.png?alt=media&token=7c21374e-af2b-419d-aa9d-238989008e6b)

15. 모든 *Episode*에 대하여 학습을 완료한 상태이다. 

**즉, 에이전트가 학습을 완료한 상태가 된다.**

---

## 3. Summary

지금까지 *Bellman Equation*에서 시작하여, 가치함수, *Q-learning*, 그리고 *Deep Q-learning*의 기본개념을 살펴보았다. 다음 챕터에서는 **PyTorch**와 OpenAI에서 제작한 모듈인 **gym**을 사용하여 *Deep Q-learning* 알고리즘을 바탕으로 실제로 에이전트를 학습 해보겠다.
